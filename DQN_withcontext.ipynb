{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# sys.path.append(\"D:\\thesis\\thesis\\rllib\")\n",
    "\n",
    "import rllib as rl\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "import torch\n",
    "dataset_train = \"./data_preprossing/output/train_cleaned.csv\"\n",
    "dataset_test = \"./data_preprossing/output/test_cleaned.csv\"\n",
    "symptom_nlice_file =\"./data/basic/symptom_nlice.json\"\n",
    "# symptom_map_file = \"./data/basic/symptoms_db.json\"\n",
    "symptom_map_file = \"./data_preprossing/output/basic/symptom_db_v1.json\"\n",
    "condition_map_file = \"./data/basic/conditions_db.json\"\n",
    "body_parts=\"./data_preprossing/output/basic/body-parts-enc.json\"\n",
    "excitation_enc=\"./data_preprossing/output/basic/excitation_encoding.json\"\n",
    "frequency_enc=\"./data_preprossing/output/basic/frequency_encoding.json\"\n",
    "nature_enc=\"./data_preprossing/output/basic/nature_encoding.json\"\n",
    "vas_enc=\"./data_preprossing/output/basic/vas_encoding.json\"\n",
    "onset_enc=\"./data_preprossing/output/basic/onset_encoding.json\"\n",
    "duration_enc=\"./data_preprossing/output/basic/duration_encoding.json\"\n",
    "# clf_file = \"data/basic/data/output/rf/rf_clf.joblib\"\n",
    "# clf_file = \"data/basic/data/output/rf/rf_clf.joblib\"\n",
    "clf_file = \"./symtom_models/output/nb_nlice/nb_serialized_sparse.joblib\"\n",
    "clf_data = joblib.load(clf_file)\n",
    "clf = clf_data.get('clf')\n",
    "import datetime\n",
    "import sys\n",
    "from pathlib import Path\n",
    "curr_path = str(Path().absolute())\n",
    "parent_path = str(Path().absolute().parent)\n",
    "sys.path.append(parent_path) # add current terminal path to sys.path\n",
    "curr_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") # obtain current time\n",
    "test = \"./data_preprossing/output/data/total_1k_min3.csv\"\n",
    "test_v1 = \"./data_preprossing/output/data/total_50.csv\"\n",
    "# with open(symptom_map_file) as fp:\n",
    "#             symptoms = json.load(fp)\n",
    "#             sorted_symptoms = sorted(symptoms.keys())\n",
    "#             symptom_map = {code: idx for idx, code in enumerate(sorted_symptoms)}\n",
    "# with open(symptom_map_file, \"w\") as fp:\n",
    "#     json.dump(symptom_map, fp, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = rl.environment.RLBasicMedEnvContext(\n",
    "    data_file=test,\n",
    "    symptom_map_file=symptom_map_file,\n",
    "    condition_map_file=condition_map_file,\n",
    "    body_parts=body_parts,\n",
    "    excitation_enc=excitation_enc,\n",
    "    frequency_enc=frequency_enc,\n",
    "    nature_enc=nature_enc, \n",
    "    vas_enc=vas_enc, \n",
    "    onset_enc=onset_enc, \n",
    "    duration_enc=duration_enc,\n",
    "    clf=clf,\n",
    "    classifer=\"nb\",\n",
    "    max_turn=20,\n",
    "    epoch =16000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61\n"
     ]
    }
   ],
   "source": [
    "input_dim = env.num_symptoms*8\n",
    "output_dim = env.num_symptoms + 33\n",
    "print(env.num_symptoms)\n",
    "layer_config = [\n",
    "    [input_dim, 128],\n",
    "    [128, 64],\n",
    "    [64, 48],\n",
    "    [48, 32],\n",
    "    [32, 32],\n",
    "    [32, 48],\n",
    "    [48, 64],\n",
    "    [64, output_dim]\n",
    "]\n",
    "\n",
    "learning_start = 1280\n",
    "batch_size = 64\n",
    "target_update = 1280\n",
    "replay_capacity = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = rl.agent.MedAgent(\n",
    "    env,\n",
    "    layer_config=layer_config,\n",
    "    learning_start=learning_start,\n",
    "    batch_size=batch_size,\n",
    "    target_update=target_update,\n",
    "    replay_capacity=replay_capacity,\n",
    "    debug=False,\n",
    "    train = True, \n",
    "    input_dim = input_dim,\n",
    "    output_dim = output_dim,\n",
    "    context = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench = rl.bench.MedBench(agent,33000)\n",
    "bench.run_trial(debug=False)\n",
    "cfg = rl.config.DQNConfig()\n",
    "rl.common.make_dir(cfg.result_path,cfg.model_path)\n",
    "agent.save(cfg.model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5380303030303031\n",
      "36.97690909090909 3.036\n",
      "11262\n",
      "0.703875\n",
      "41.771 2.1716875\n",
      "0.5674375\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cfg = rl.config.DQNConfig()\n",
    "rewards,success,average_rewards,average_steps,loss =bench.rewardList,bench.get_success_rate(),bench.get_average_rewards(),bench.get_average_turn(),bench.get_loss()\n",
    "print(success)\n",
    "print(average_rewards,average_steps)\n",
    "env1 = rl.environment.RLBasicMedEnvContext(\n",
    "    data_file=test,\n",
    "    symptom_map_file=symptom_map_file,\n",
    "    condition_map_file=condition_map_file,\n",
    "    symptom_nlice_file=symptom_nlice_file,\n",
    "    body_parts=body_parts,\n",
    "    excitation_enc=excitation_enc,\n",
    "    frequency_enc=frequency_enc,\n",
    "    nature_enc=nature_enc, \n",
    "    vas_enc=vas_enc, \n",
    "    onset_enc=onset_enc, \n",
    "    duration_enc=duration_enc,\n",
    "    clf=clf,\n",
    "    classifer=\"nb\",\n",
    "    max_turn=20,\n",
    "    epoch = 16000\n",
    ")\n",
    "# agent.policy_network.load_state_dict(torch.load('./outputs/DQN/20211125-160309/models/dqn_checkpoint_policy.pth'))\n",
    "agent.env = env1\n",
    "agent.train = False\n",
    "agent.eps_start = 0\n",
    "agent.eps_end = 0\n",
    "agent.debug = False\n",
    "testbench = rl.bench.MedBenchEval(agent,16000)\n",
    "testbench.run_trial(debug=False)\n",
    "rewards,success,average_rewards,average_steps,hint_rate =testbench.rewardList,testbench.get_success_rate(),testbench.get_average_rewards(),testbench.get_average_turn(),testbench.get_hint_rate()\n",
    "print(testbench.success)\n",
    "print(success)\n",
    "print(average_rewards,average_steps)\n",
    "print(hint_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_test = rl.agent.MedAgent(\n",
    "    env1,\n",
    "    layer_config=layer_config,\n",
    "    learning_start=learning_start,\n",
    "    batch_size=batch_size,\n",
    "    target_update=target_update,\n",
    "    replay_capacity=replay_capacity,\n",
    "    debug=False,\n",
    "    train = False\n",
    ")\n",
    "# torch.save(agent.policy_network.state_dict(), cfg.model_path+'/dqn_checkpoint_policy.pth')\n",
    "# agent_test.policy_network.load_state_dict(agent.target_network.state_dict())\n",
    "# agent_test.policy_network.load_state_dict(agent.policy_network.state_dict())\n",
    "# for target_param, param in zip(agent_test.policy_network.parameters(),agent.policy_network.parameters()): # 复制参数到目标网路targe_net\n",
    "#             target_param.data.copy_(param.data)\n",
    "# for target_param, param in zip(agent_test.policy_network.parameters(),agent.target_network.parameters()): # 复制参数到目标网路targe_net\n",
    "#             target_param.data.copy_(param.data)\n",
    "agent_test.eps_start = 0\n",
    "agent_test.eps_end = 0\n",
    "# print(cfg.model_path)\n",
    "# agent_test.load(cfg.model_path)\n",
    "agent_test.policy_network.load_state_dict(torch.load('./outputs/DQN/20211123-114930/models/dqn_checkpoint_policy.pth'))\n",
    "testbench = rl.bench.MedBenchEval(agent_test,1000)\n",
    "testbench.run_trial(debug=False)\n",
    "rewards,success,average_rewards,average_steps =testbench.rewardList,testbench.get_success_rate(),testbench.get_average_rewards(),testbench.get_average_turn()\n",
    "print(success)\n",
    "print(average_rewards,average_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"./data_preprossing/output/datas/min3/Asthma.csv\"\n",
    "env1 = rl.environment.RLBasicMedEnvNOPATIENT(\n",
    "    data_file=test,\n",
    "    symptom_map_file=symptom_map_file,\n",
    "    condition_map_file=condition_map_file,\n",
    "    symptom_nlice_file=symptom_nlice_file,\n",
    "    body_parts=body_parts,\n",
    "    excitation_enc=excitation_enc,\n",
    "    frequency_enc=frequency_enc,\n",
    "    nature_enc=nature_enc, \n",
    "    vas_enc=vas_enc, \n",
    "    onset_enc=onset_enc, \n",
    "    duration_enc=duration_enc,\n",
    "    clf=clf,\n",
    "    classifer=\"nb\",\n",
    "    max_turn=20,\n",
    "    epoch = 1000\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "-18.0 21.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "agent.env = env1\n",
    "agent.train = False\n",
    "agent.eps_start = 0\n",
    "agent.eps_end = 0\n",
    "agent.debug = False\n",
    "testbench = rl.bench.MedBenchEval(agent,1000)\n",
    "testbench.run_trial(debug=False)\n",
    "rewards,success,average_rewards,average_steps,hint_rate =testbench.rewardList,testbench.get_success_rate(),testbench.get_average_rewards(),testbench.get_average_turn(),testbench.get_hint_rate()\n",
    "print(success)\n",
    "print(average_rewards,average_steps)\n",
    "print(hint_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AiMedPatient(symptoms=array([ 0,  0,  0,  0,  0,  0,  0,  0,  1, 11,  1,  1,  1,  1,  1,  1,  0,\n",
      "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "        0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "        0,  0,  0,  0,  1,  1,  1,  1,  1,  1,  1,  1,  0,  0,  0,  0,  0,\n",
      "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0], dtype=uint16), condition=8)\n",
      "8\n",
      "[  8  10  11  12  13  14  15 128 129 130 131 132 133 134 135 208 209 210\n",
      " 211 212 213 214 215]\n",
      "[1.0, 16.0, 26.0]\n",
      "{0, 16, 42, 28}\n",
      "0.0\n",
      "20.0 4.0\n"
     ]
    }
   ],
   "source": [
    "def find_symptom(index):\n",
    "        symptoms=[]\n",
    "        for s in index:\n",
    "            if(s%8==0):\n",
    "                symptoms.append(s/8)\n",
    "        return symptoms\n",
    "\n",
    "\n",
    "agent.env = env1\n",
    "agent.train = False\n",
    "agent.eps_start = 0\n",
    "agent.eps_end = 0\n",
    "agent.debug = False\n",
    "testbench = rl.bench.MedBenchEval(agent,1)\n",
    "testbench.run_trial(debug=False)\n",
    "print(agent.env.patient)\n",
    "print(agent.env.patient.condition)\n",
    "print(np.where(agent.env.patient.symptoms==1)[0])\n",
    "print(find_symptom(np.where(agent.env.patient.symptoms==1)[0]))\n",
    "print(agent.env.inquiry_list)\n",
    "rewards,success,average_rewards,average_steps =testbench.rewardList,testbench.get_success_rate(),testbench.get_average_rewards(),testbench.get_average_turn()\n",
    "print(success)\n",
    "print(average_rewards,average_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "def plot_loss(loss,tag=\"train\",env=\"milestone1\",algo = \"DQN\"):\n",
    "    sns.set()\n",
    "    plt.title(\"average learning curve of {} for {}\".format(algo,env))\n",
    "    plt.xlabel('epsiodes')\n",
    "    plt.plot(loss,label='loss')\n",
    "    plt.legend()\n",
    "\n",
    "\n",
    "def plot_rewards(rewards,tag=\"train\",env=\"milestone1\",algo = \"DQN\"):\n",
    "    sns.set()\n",
    "    plt.title(\"average learning curve of {} for {}\".format(algo,env))\n",
    "    plt.xlabel('epsiodes')\n",
    "    plt.plot(rewards,label='reward')\n",
    "    plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_rewards(rewards)\n",
    "eval = rewards\n",
    "success = 0\n",
    "plot_rewards(eval)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7a49119d25f186b1b8fbf45c92b26e2bab72f5183a7a97ce39124ca72ff0752f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
