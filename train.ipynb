{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook trains an RF and NB agent on the extended data.\n",
    "\n",
    "A separate dataset would be used to train the RL agent.\n",
    "\n",
    "The downside to this method is that the RL method relies heavily on the accuracy of the RF and NB models and would be limited by their accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#module_dir = \"/Users/teliov/TUD/symcat-to-synthea/output/module_ai_med_extended\"\n",
    "module_dir = \"/home/gzr/文档/medvice/thesis/data/NLICE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thesislib.utils.ml import process\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import pathlib\n",
    "import json\n",
    "import os\n",
    "from thesislib.utils.ml import process\n",
    "from thesislib.utils.ml import runners, models\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symptom_map, condition_map = process.get_symptom_condition_map(module_dir)\n",
    "print(symptom_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ata_dir = \"/Users/teliov/TUD/Thesis/Medvice/Notebooks/data/06_18_nlice_plus/extended\"\n",
    "data_dir = \"/home/gzr/文档/medvice/thesis/data/basic\"\n",
    "pathlib.Path(data_dir).mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symptom_map_file = os.path.join(data_dir, \"symptom_db.json\")\n",
    "with open(symptom_map_file, \"w\") as fp:\n",
    "    json.dump(symptom_map, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_map_file = os.path.join(data_dir, \"condition_db.json\")\n",
    "with open(condition_map_file, \"w\") as fp:\n",
    "    json.dump(condition_map, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thesislib.utils.ml import process\n",
    "from thesislib.utils.ml import runners, models\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_csv = \"/Users/teliov/TUD/Thesis/Medvice/Notebooks/data/06_18_nlice_plus/ai/output_med_ai_ext/symptoms/csv/symptoms.csv\"\n",
    "data_csv = \"/home/gzr/文档/medvice/thesis/data/nlice100k/symptoms/csv/symptoms.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(symptom_file, output_path, use_headers=False, train_split=0.8):\n",
    "    symptom_columns = ['PATIENT', 'GENDER', 'RACE', 'ETHNICITY', 'AGE_BEGIN', 'AGE_END',\n",
    "                       'PATHOLOGY', 'NUM_SYMPTOMS', 'SYMPTOMS']\n",
    "\n",
    "    pathlib.Path(output_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if use_headers:\n",
    "        df = pd.read_csv(symptom_file, names=symptom_columns)\n",
    "    else:\n",
    "        df = pd.read_csv(symptom_file,sep = \"\\t\")\n",
    "    df.index.name = \"Index\" \n",
    "    labels = df[\"PATHOLOGY\"].drop(0)\n",
    "    datas = df.drop(0)\n",
    "    print(labels)\n",
    "    #print(labels.value_counts())\n",
    "    splitter = StratifiedShuffleSplit(1, train_size=train_split)\n",
    "    train_index = None\n",
    "    test_index = None\n",
    "    for tr_idx, tst_index in splitter.split(datas, labels):\n",
    "        train_index = tr_idx\n",
    "        test_index = tst_index\n",
    "        break\n",
    "\n",
    "    train_df = df.iloc[train_index]\n",
    "    test_df = df.iloc[test_index]\n",
    "\n",
    "    train_op = os.path.join(output_path, \"train.csv\")\n",
    "    test_op = os.path.join(output_path, \"test.csv\")\n",
    "    train_df.to_csv(train_op)\n",
    "    test_df.to_csv(test_op)\n",
    "    return train_op, test_op\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op_data_dir = os.path.join(data_dir, \"data\")\n",
    "# split into train and test\n",
    "train_file, test_file = split_data(data_csv, op_data_dir,True,train_split=0.9)\n",
    "\n",
    "parsed_data_dir = os.path.join(op_data_dir, \"parsed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parse the train set\n",
    "RACE_CODE = {'white': 0, 'black':1, 'asian':2, 'native':3, 'other':4}\n",
    "def parse_data_nlice_adv(\n",
    "        filepath,\n",
    "        conditions_db_json,\n",
    "        symptoms_db_json,\n",
    "        output_path,\n",
    "        body_parts_json,\n",
    "        excitation_enc_json,\n",
    "        frequency_enc_json,\n",
    "        nature_enc_json,\n",
    "        vas_enc_json,\n",
    "        onset_json,\n",
    "        duration_json\n",
    "        ):\n",
    "    pathlib.Path(output_path).mkdir(parents=True, exist_ok=True)\n",
    "    with open(symptoms_db_json) as fp:\n",
    "        symptoms_db = json.load(fp)\n",
    "\n",
    "    with open(conditions_db_json) as fp:\n",
    "        conditions_db = json.load(fp)\n",
    "\n",
    "    with open(body_parts_json) as fp:\n",
    "        body_parts = json.load(fp)\n",
    "\n",
    "    with open(excitation_enc_json) as fp:\n",
    "        excitation_enc = json.load(fp)\n",
    "\n",
    "    with open(frequency_enc_json) as fp:\n",
    "        frequency_enc = json.load(fp)\n",
    "\n",
    "    with open(nature_enc_json) as fp:\n",
    "        nature_enc = json.load(fp)\n",
    "\n",
    "    with open(vas_enc_json) as fp:\n",
    "        vas_enc = json.load(fp)\n",
    "\n",
    "    with open(onset_json) as fp:\n",
    "        onset_enc = json.load(fp)\n",
    "\n",
    "    with open(duration_json) as fp:\n",
    "        duration_enc = json.load(fp)\n",
    "\n",
    "    usecols = ['GENDER', 'RACE', 'AGE_BEGIN', 'PATHOLOGY', 'NUM_SYMPTOMS', 'SYMPTOMS']\n",
    "\n",
    "    df = pd.read_csv(filepath, usecols=usecols)\n",
    "    filename = filepath.split(\"/\")[-1]\n",
    "\n",
    "    # drop the guys that have no symptoms\n",
    "    #f['NUM_SYMPTOMS'] = df['NUM_SYMPTOMS'].astype(int)\n",
    "    #df = df[df['NUM_SYMPTOMS']> 0]\n",
    "    df['LABEL'] = df.PATHOLOGY.apply(lambda v: conditions_db.get(v))\n",
    "    df['RACE'] = df.RACE.apply(lambda v: RACE_CODE.get(v))\n",
    "    df['GENDER'] = df.GENDER.apply(lambda gender: 0 if gender == 'F' else 1)\n",
    "    df = df.rename(columns={'AGE_BEGIN': 'AGE'})\n",
    "    # print(df.SYMPTOMS)\n",
    "    df['SYMPTOMS'] = df.SYMPTOMS.apply(\n",
    "        transform_symptoms_nlice_adv,\n",
    "        symptom_db=symptoms_db,\n",
    "        body_parts=body_parts,\n",
    "        excitation_enc=excitation_enc,\n",
    "        frequency_enc=frequency_enc,\n",
    "        nature_enc=nature_enc,\n",
    "        vas_enc=vas_enc,\n",
    "        onset_enc = onset_enc,\n",
    "        duration_enc = duration_enc\n",
    "    )\n",
    "\n",
    "    ordered_keys = ['LABEL', 'GENDER', 'RACE', 'AGE', 'SYMPTOMS']\n",
    "    df = df[ordered_keys]\n",
    "    df.index.name = \"Index\"\n",
    "    output_file = os.path.join(output_path, \"%s_sparse.csv\" % filename)\n",
    "    df.to_csv(output_file)\n",
    "\n",
    "    return output_file\n",
    "\n",
    "def transform_symptoms_nlice_adv(\n",
    "        symptom_str,\n",
    "        symptom_db,\n",
    "        body_parts,\n",
    "        excitation_enc,\n",
    "        frequency_enc,\n",
    "        nature_enc,\n",
    "        vas_enc,\n",
    "        onset_enc,\n",
    "        duration_enc\n",
    "):  \n",
    "    # print(symptom_str)\n",
    "    symptom_list = symptom_str.split(\";\")\n",
    "    transformed_symptoms = []\n",
    "    for _symp_def in symptom_list:\n",
    "        # print(_symp_def)\n",
    "        sym_list = _symp_def.split(\":\")\n",
    "        if(len(sym_list)==9):\n",
    "            _symptom, _nature, _location, _intensity, _duration, _onset, _exciation, _frequency, _ = _symp_def.split(\":\")\n",
    "\n",
    "            _symptom_idx = symptom_db[_symptom] * 8\n",
    "\n",
    "            _nature_idx = _symptom_idx + 1\n",
    "            _nature_val = 1 if _nature == \"\" or _nature == \"other\" else nature_enc.get(_nature)\n",
    "\n",
    "            _location_idx = _symptom_idx + 2\n",
    "            _location_val = 1 if _location == \"\" or _location == \"other\" else body_parts.get(_location)\n",
    "\n",
    "            _intensity_idx = _symptom_idx + 3\n",
    "            _intensity_val = 1 if _intensity == \"\" else vas_enc.get(_intensity)\n",
    "\n",
    "            _duration_idx = _symptom_idx + 4\n",
    "            _duration_val = 0 if _duration == \"\" else duration_enc.get(_duration)\n",
    "\n",
    "            _onset_idx = _symptom_idx + 5\n",
    "            _onset_val = 0 if _onset == \"\" else onset_enc.get(_onset)\n",
    "\n",
    "            _excitation_idx = _symptom_idx + 6\n",
    "            _excitation_val = 1 if _exciation == \"\" else excitation_enc.get(_exciation)\n",
    "\n",
    "            _frequency_idx = _symptom_idx + 7\n",
    "            _frequency_val = 1 if _frequency == \"\" else frequency_enc.get(_frequency)\n",
    "\n",
    "            to_transform = [\n",
    "                \"|\".join([str(_symptom_idx), \"1\"]),\n",
    "                \"|\".join([str(_nature_idx), str(_nature_val)]),\n",
    "                \"|\".join([str(_location_idx), str(_location_val)]),\n",
    "                \"|\".join([str(_intensity_idx), str(_intensity_val)]),\n",
    "                \"|\".join([str(_excitation_idx), str(_excitation_val)]),\n",
    "                \"|\".join([str(_frequency_idx), str(_frequency_val)])\n",
    "            ]\n",
    "\n",
    "            if _duration_val != 0:\n",
    "                to_transform.append(\n",
    "                    \"|\".join([str(_duration_idx), str(_duration_val)])\n",
    "                )\n",
    "\n",
    "            if _onset_val != 0:\n",
    "                to_transform.append(\n",
    "                    \"|\".join([str(_onset_idx), str(_onset_val)]),\n",
    "                )\n",
    "\n",
    "            transformed_str = \";\".join(to_transform)\n",
    "\n",
    "            transformed_symptoms.append(transformed_str)\n",
    "\n",
    "    return \";\".join(transformed_symptoms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the train set and let's train\n",
    "parsed_train = \"./data/basic/data/parsed/train_csv_sparse.csv\"\n",
    "symptom_map_file = \"./data/basic/symptoms_db.json\"\n",
    "condition_map_file = \"./data/basic/conditions_db.json\"\n",
    "dataset_train = \"./data_preprossing/output/train_cleaned.csv\"\n",
    "dataset_test = \"./data_preprossing/output/test_cleaned.csv\"\n",
    "symptoms_db_file = \"./data/basic/symptoms_db.json\"\n",
    "body_parts_file = \"./data/basic/body-parts-enc.json\"\n",
    "excitation_enc_file = \"./data/basic/excitation_encoding.json\"\n",
    "frequency_enc_file = \"./data/basic/frequency_encoding.json\"\n",
    "nature_enc_file = \"./data/basic/nature_encoding.json\"\n",
    "vas_enc_file = \"./data/basic/vas_encoding.json\"\n",
    "onset_enc_file = \"./data/basic/onset_encoding.json\"\n",
    "duration_enc_file = \"./data/basic/duration_encoding.json\"\n",
    "parsed_data_dir = \"./data/basic/data/parsed\"\n",
    "        # excitation_enc_json,\n",
    "        # frequency_enc_json,\n",
    "        # nature_enc_json,\n",
    "        # vas_enc_json\n",
    "parsed_train = parse_data_nlice_adv(\n",
    "    dataset_test,\n",
    "    condition_map_file,\n",
    "    symptoms_db_file,\n",
    "    parsed_data_dir,\n",
    "    body_parts_file,\n",
    "    excitation_enc_file,\n",
    "    frequency_enc_file,\n",
    "    nature_enc_file,\n",
    "    vas_enc_file,\n",
    "    onset_enc_file,\n",
    "    duration_enc_file\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "RACE_CODE = {'white': 0, 'black':1, 'asian':2, 'native':3, 'other':4}\n",
    "def _symptom_transform(val, labels, is_nlice=False):\n",
    "    \"\"\"\n",
    "    Val is a string in the form: \"symptom_0;symptom_1;...;symptom_n\"\n",
    "    :param val:\n",
    "    :param labels:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    parts = val.split(\";\")\n",
    "    if is_nlice:\n",
    "        indices = []\n",
    "        for item in parts:\n",
    "            id, enc = item.split(\"|\")\n",
    "            label = labels.get(id)\n",
    "            indices.append(\"|\".join([label, enc]))\n",
    "        res = \",\".join(indices)\n",
    "    else:\n",
    "        indices = []\n",
    "        for item in parts:\n",
    "            symptom,_,_,_,_,_,_,_,_ = item.split(\":\")\n",
    "            id = labels.get(symptom)\n",
    "            if _ is None:\n",
    "                raise ValueError(\"Unknown symptom\")\n",
    "            indices.append(id)\n",
    "        res = \",\".join(indices)\n",
    "    return res\n",
    "\n",
    "\n",
    "def parse_data(\n",
    "        filepath,\n",
    "        conditions_db_json,\n",
    "        symptoms_db_json,\n",
    "        output_path,\n",
    "        is_nlice=False,\n",
    "        transform_map=None,\n",
    "        encode_map=None,\n",
    "        reduce_map=None):\n",
    "\n",
    "    pathlib.Path(output_path).mkdir(parents=True, exist_ok=True)\n",
    "    with open(symptoms_db_json) as fp:\n",
    "        symptoms_db = json.load(fp)\n",
    "\n",
    "    with open(conditions_db_json) as fp:\n",
    "        conditions_db = json.load(fp)\n",
    "\n",
    "    condition_labels = {code: idx for idx, code in enumerate(sorted(conditions_db.keys()))}\n",
    "    symptom_map = {code: str(idx) for idx, code in enumerate(sorted(symptoms_db.keys()))}\n",
    "\n",
    "    usecols = ['GENDER', 'RACE', 'AGE_BEGIN', 'PATHOLOGY', 'NUM_SYMPTOMS', 'SYMPTOMS']\n",
    "\n",
    "    df = pd.read_csv(filepath, usecols=usecols)\n",
    "\n",
    "    filename = filepath.split(\"/\")[-1]\n",
    "\n",
    "    # drop the guys that have no symptoms\n",
    "    df = df[df.NUM_SYMPTOMS > 0]\n",
    "    df['LABEL'] = df.PATHOLOGY.apply(lambda v: condition_labels.get(v))\n",
    "    df['RACE'] = df.RACE.apply(lambda v: RACE_CODE.get(v))\n",
    "    df['GENDER'] = df.GENDER.apply(lambda gender: 0 if gender == 'F' else 1)\n",
    "    df = df.rename(columns={'AGE_BEGIN': 'AGE'})\n",
    "    # if is_nlice:\n",
    "    #     df['SYMPTOMS'] = df.SYMPTOMS.apply(\n",
    "    #         _tranform_symptoms,\n",
    "    #         transformation_map=transform_map,\n",
    "    #         symptom_combination_encoding_map=encode_map,\n",
    "    #         reduction_map=reduce_map)\n",
    "    df['SYMPTOMS'] = df.SYMPTOMS.apply(_symptom_transform, labels=symptom_map, is_nlice=is_nlice)\n",
    "    ordered_keys = ['LABEL', 'GENDER', 'RACE', 'AGE', 'SYMPTOMS']\n",
    "    df = df[ordered_keys]\n",
    "    df.index.name = \"Index\"\n",
    "\n",
    "    output_file = os.path.join(output_path, \"%s_sparse.csv\" % filename)\n",
    "    df.to_csv(output_file)\n",
    "\n",
    "    return output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the test set for evaluation\n",
    "# parse the train set and let's train\n",
    "dataset_train = \"./data_preprossing/output/train_cleaned.csv\"\n",
    "dataset_test = \"./data_preprossing/output/test_cleaned.csv\"\n",
    "parsed_data_dir = \"./data/basic/data/parsed\"\n",
    "parsed_test = parse_data(\n",
    "    dataset_train,\n",
    "    condition_map_file,\n",
    "    symptom_map_file,\n",
    "    parsed_data_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n"
     ]
    }
   ],
   "source": [
    "# train with RF and then with NB\n",
    "data_dir = os.curdir\n",
    "print(data_dir)\n",
    "op_data_dir = os.path.join(data_dir, \"symtom_models\")\n",
    "rf_dir = os.path.join(op_data_dir, \"output/rf\")\n",
    "rfparams = models.RFParams()\n",
    "rfparams.n_estimators = 200\n",
    "rfparams.max_depth = None\n",
    "parsed_train = \"./data/basic/data/parsed/train_cleaned.csv_sparse.csv\"\n",
    "symptom_map_file = \"./data/basic/symptoms_db.json\"\n",
    "condition_map_file = \"./data/basic/conditions_db.json\"\n",
    "pathlib.Path(rf_dir).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_ok = runners.train_ai_med_rf(\n",
    "    parsed_train,\n",
    "    symptom_map_file,\n",
    "    rf_dir,\n",
    "    rfparams,\n",
    "    \"Basic AI-MED Run\",\n",
    "    \"local-pc\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24, 29, 49, 38, 5, 3, 43, 48, 41, 37, 12, 31, 9, 42, 35, 28, 8, 14, 36, 4, 40, 19, 20, 21, 39, 53, 10, 25, 0, 44, 2, 17, 45]\n"
     ]
    }
   ],
   "source": [
    "# train NB\n",
    "nb_dir = os.path.join(op_data_dir, \"output/nb\")\n",
    "\n",
    "run_ok = runners.train_ai_med_nb(\n",
    "    parsed_train,\n",
    "    symptom_map_file,\n",
    "    nb_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24, 29, 49, 38, 5, 3, 43, 48, 41, 37, 12, 31, 9, 42, 35, 28, 8, 14, 36, 4, 40, 19, 20, 21, 39, 53, 10, 25, 0, 44, 2, 17, 45]\n"
     ]
    }
   ],
   "source": [
    "rf_dir = os.path.join(op_data_dir, \"output/rf\")\n",
    "\n",
    "run_ok = runners.train_ai_med_adv_rf(\n",
    "    parsed_train,\n",
    "    symptom_map_file,\n",
    "    rf_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll evaluate on the unseen data .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nb_data = joblib.load(nb_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_clf = nb_data.get('clf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec=[[1,0,44,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
    ",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
    ",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
    ",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
    ",1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
    ",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0\n",
    ",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
    ",0,0,0,0,0,0,0,0,0,0,0,126,0,0,0,0,0,0\n",
    ",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
    ",0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0\n",
    ",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
    ",0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0\n",
    ",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
    ",0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
    ",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = np.array(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csc_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = csc_matrix(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = nb_clf.predict_proba(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5cd40725cd6b1307bf83bc36da4507d9aed23032251df09ae5af98802e40880b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
